{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling up neural net training in PyTorch using distributed data parallel training\n",
    "\n",
    "## Neural network training basic setup\n",
    "- Before covering the distributed data parallel algorithm it's important to understand the basics of neural network training in PyTorch, its bottlenecks, and the natural progression from training on a cpu to training across multiple machines as data and models grow. \n",
    "\n",
    "**Before getting started**: This notebook can be run only on CPUs for the most part, and you only need a working installation of python and pytorch, no other dependencies! At some point, support for some operating systems will stop (when the distributed portion starts), later, support for CPU-only training will stop. I have tried my best to make the code as readable as possible, so even if you can't run it, hopefully it still illustrates the concepts.\n",
    "To run all the code here you will need access to a machine with multiple GPUs. Luckily, UH has an excellent [HPC cluster](https://datascience.hawaii.edu/hpc/). Getting an account is free and easy for students and it's a great resource!\n",
    "\n",
    "\n",
    "### A gentle introduction on the CPU\n",
    "- Generally, neural networks can be trained on any hardware, as such, the CPU is the natural starting point that is accessible to anyone.\n",
    "- We will start with a small network and toy data to introduce the problem and training procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10bf930b0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from statistics import mean\n",
    "from typing import List, Tuple\n",
    "\n",
    "torch.manual_seed(691)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model class\n",
    "class FFNet(nn.Module):\n",
    "    \"\"\"Simple neural net class.\n",
    "\n",
    "    Goal is not to be comprehensive but rather to provide a simple interface to a toy network that allows to quickly scale the size of the network. \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size: int = 100,\n",
    "                 num_hl: int = 0,\n",
    "                 hl_size: int = 10):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size (int): dimensionality of the input. Default is 100.\n",
    "            num_hl (int): Number of hidden layers (layers between input and output layer) in the network. Default is 0.\n",
    "            hl_size (int): Number of units per hidden layer. Default is 10.\n",
    "        \"\"\"\n",
    "        super(FFNet, self).__init__()\n",
    "        # initial fully connected layer that ingests input\n",
    "        self._fc_in = nn.Linear(input_size, hl_size)\n",
    "        self._relu = nn.ReLU()\n",
    "\n",
    "        # intermediate hidden layers, easily allow us to grow the network\n",
    "        self._hidden_layers = None\n",
    "        if num_hl > 0:\n",
    "            hls = []\n",
    "            for _ in range(num_hl):\n",
    "                hls += [nn.Linear(hl_size, hl_size), nn.ReLU()]\n",
    "            self._hidden_layers = nn.Sequential(*hls)\n",
    "\n",
    "        # output layer, no activation function for simplicity\n",
    "        self._fc_out = nn.Linear(hl_size, 1)\n",
    "\n",
    "    def forward(self, x: torch.tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass of data through network.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input data. Shape must be Bxinput_size, where B is the batch size and input_size is the data dimensionality.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Inputs after being processed by the network. Shape will be Bx1 where 1 is the width of the output layer.\n",
    "        \"\"\"\n",
    "        z = self._fc_in(x)\n",
    "        z = self._relu(z)\n",
    "        if self._hidden_layers:\n",
    "            z = self._hidden_layers(z)\n",
    "        y = self._fc_out(z)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data class\n",
    "class ToyDataset:\n",
    "    \"\"\"Simple dataset class.\n",
    "\n",
    "    Creates artificial data, goal is to easily stream data without dataloading being a bottleneck. \n",
    "    Should be passed to torch.utils.data.DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_size: int = 100,\n",
    "                 num_samples: int = 1_024):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            input_size (int): Dimensionality of the data\n",
    "            num_samples (int): Simulated size of the dataset\n",
    "        \"\"\"\n",
    "        self._input_size = input_size\n",
    "        self._num_samples = num_samples\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            int: Number of samples in the dataset\n",
    "        \"\"\"\n",
    "        return self._num_samples\n",
    "\n",
    "    def __getitem__(self,\n",
    "                    index: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            index (int): Ignored since data is randomly generated on the fly.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[torch.Tensor, torch.Tensor]: sample, label data pair\n",
    "        \"\"\"\n",
    "        return torch.randn(self._input_size), torch.randn(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set up training on the CPU\n",
    "- No special hardware required\n",
    "- Not parallelized at all\n",
    "- Creating the model and the dataset:\n",
    "    - In a real-world application, you'd want a training and validation dataset as we've discussed in class before. \n",
    "    - However, since we're only interested in performance here and the data is random anyway, a training dataset will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "batch_size = 8\n",
    "input_size = 100\n",
    "num_samples = 1_024\n",
    "\n",
    "train_ds = ToyDataset(input_size=input_size, num_samples=num_samples)\n",
    "# Using the torch DataLoader is good practice as it allows easy shuffling and scaling to parallel data loading by increasing the number of workers\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "num_hl = 0\n",
    "hl_size = 10\n",
    "\n",
    "nn_mdl = FFNet(input_size=input_size, num_hl=num_hl, hl_size=hl_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training the model\n",
    "    * To train the model we need to create a loss function to indicate to the model how \"well\" it's doing. We will use the mean squared error, but it doesn't really matter here. \n",
    "    * We also need to define a optimizer to update the model parameters over the course of training. We will use standard stochastic gradient descent with momentum. More on that in a bit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(nn_mdl.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 - total loss: 0.061\n",
      "Epoch: 2 - total loss: 0.064\n",
      "Epoch: 3 - total loss: 0.064\n",
      "Epoch: 4 - total loss: 0.063\n",
      "Epoch: 5 - total loss: 0.063\n",
      "Epoch: 6 - total loss: 0.065\n",
      "Epoch: 7 - total loss: 0.069\n",
      "Epoch: 8 - total loss: 0.062\n",
      "Epoch: 9 - total loss: 0.060\n",
      "Epoch: 10 - total loss: 0.063\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# how many times we want to loop over the training data\n",
    "nb_epochs = 10\n",
    "\n",
    "for epoch in range(nb_epochs): \n",
    "    running_loss = 0.\n",
    "    for samples, labels in train_loader:\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = nn_mdl(samples)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # we expect loss to remain pretty consistent since data and labels are random\n",
    "    print(f'Epoch: {epoch + 1} - total loss: {running_loss / 2000:.3f}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmarking\n",
    "* Since we're interested in performance, let's create a function to do the whole training process and only return the time needed to train a given model with a given dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_training(nn_mdl: nn.Module,\n",
    "                       batch_size: int = 32,\n",
    "                       input_size: int = 100,\n",
    "                       num_samples: int = 16_384,\n",
    "                       nb_epochs: int = 10,\n",
    "                       repeats: int = 10,\n",
    "                       verbose: bool = True) -> List[float]:\n",
    "    \"\"\"Function to benchmark training time for a provided model on toy data.\n",
    "\n",
    "    Args:\n",
    "        nn_mdl (nn.Module): model to be trained \n",
    "        batch_size (int, optional): Defaults to 32.\n",
    "        input_size (int, optional): Size of training data samples. Defaults to 100.\n",
    "        num_samples (int, optional): Number of training data samples. Defaults to 16_384.\n",
    "        nb_epochs (int, optional): How many epochs to train for. Defaults to 10.\n",
    "        repeats (int, optional): How many times to repeat the whole training process. Defaults to 10.\n",
    "        verbose (bool, optional): Whether to print progress between each repeat. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        List[flaot]: List of times it took for training to complete per repeat. In nanoseconds\n",
    "    \"\"\"\n",
    "    # initialize data\n",
    "    train_ds = ToyDataset(input_size=input_size, num_samples=num_samples)\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_ds, batch_size=batch_size, num_workers=0, shuffle=False)\n",
    "\n",
    "    # initialize optimizer& loss\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(nn_mdl.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    # train loop\n",
    "    train_times = []\n",
    "    for rep_i in range(repeats):\n",
    "        if verbose:\n",
    "            print(f'Repeat number: {rep_i + 1} of {repeats}')\n",
    "        start = time.perf_counter_ns()\n",
    "        for epoch in range(nb_epochs):\n",
    "            for batch_idx, data in enumerate(train_loader):\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                samples, labels = data\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                outputs = nn_mdl(samples)\n",
    "                loss = loss_fn(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        train_times.append(time.perf_counter_ns()-start)\n",
    "\n",
    "    return train_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time to train: 1.124 seconds\n"
     ]
    }
   ],
   "source": [
    "base_result = benchmark_training(nn_mdl, verbose=False)\n",
    "print(f'Average time to train: {(mean(base_result) * 1e-9):.3f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling up - more data, bigger models (I)\n",
    "* Training on a CPU will almost always work. But it will quickly stop being fast. CPUs, because they are built for universailty, sacrifice speed for very specific applications. \n",
    "* See below how training times quickly get out of hand as the models or data (or both) grow\n",
    "    * Notice especially how they don't grow additively but rather (roughly) multiplicatively. If increasing the model or data alone results in a 10x of training time, increasing both roughly results in a 100x training time, not 20x. \n",
    "* Rememer from class: large language models (LLMs) are huge, starting in the 10s-of-millions parameter range and scaling up to **trillions** of parameters. And they are trained on massive corpora. Running that kind of computation is impossible. \n",
    "    * See below for an illustration of how LLMs have grown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/llm_param_nb.jpeg\" width=\"50%\"/>\n",
    "<!--cite [2] https://huggingface.co/blog/large-language-models-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of parameters in original, small model: 1021\n",
      "Total number of parameters in medium-sized model: 40501\n",
      "Time to train medium-sized model: 15.118 seconds\n"
     ]
    }
   ],
   "source": [
    "# grow the model, keep data the same\n",
    "print(f'Total number of parameters in original, small model: {sum(p.numel() for p in nn_mdl.parameters())}')\n",
    "tmp_mdl = FFNet(input_size=input_size, num_hl=3, hl_size=100)\n",
    "print(f'Total number of parameters in medium-sized model: {sum(p.numel() for p in tmp_mdl.parameters())}')\n",
    "result = benchmark_training(tmp_mdl, verbose=False, repeats=1)\n",
    "print(f'Time to train medium-sized model: {(mean(result) * 1e-9):.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train small model on a larger amount of data: 9.450 seconds\n"
     ]
    }
   ],
   "source": [
    "# increase the amount of data to roughly 131k (2^17) samples\n",
    "result = benchmark_training(nn_mdl, num_samples=131_072, verbose=False, repeats=1)\n",
    "print(f'Time to train small model on a larger amount of data: {(mean(result) * 1e-9):.3f} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train a larger model on a larger amount of data: 100.629 seconds\n"
     ]
    }
   ],
   "source": [
    "# scaling both model size and amount of data:\n",
    "tmp_mdl = FFNet(input_size=input_size, num_hl=3, hl_size=100)\n",
    "result = benchmark_training(tmp_mdl, num_samples=131_072, verbose=False, repeats=1)\n",
    "print(f'Time to train a larger model on a larger amount of data: {(mean(result) * 1e-9):.3f} seconds')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling up - more data, bigger models (cont.)\n",
    "* Training a neural network is a computationally expensive, but highly repetitive and parallelizabl task. \n",
    "* Instead of using a CPU which is not optimized for this sort of computation, we can use hardware that is more specifically built to excell at the most repetitive componenets of training a neural network:\n",
    "#### GPUs\n",
    "* One easy way to scale up is through dedicated hardware that is able to train the neural networks faster/ more efficiently.\n",
    "* GPUs, intially built for computer graphics applications (hence the name graphical processing unit) are very adapt at quickly computing large matrix operations, which are the basis for neural network training. Furthermore, GPUs have significantly more memory throughput than CPUs, allowing for more data to be processed. \n",
    "* In essence, when training on a GPU, the entire model is held in memory, data can be passed through the model quickly and results are stored in the GPU too to efficiently calcuate gradients and make parameter updates. \n",
    "* Training on one (or multiple) GPUs is efficient, but it requires enough memory to fit the whole model in the GPU and store the results of forward passes of the data as well. \n",
    "* So, when training large models with high-dimensional data and large batch sizes, even a single GPU might not be enough. \n",
    "* Usually, we run into one of two problems, that deserve closer examinatino:\n",
    "    1) **We cannot make our batches of data large enough**\n",
    "    2) **We can't even fit the entire model into GPU memory**\n",
    "\n",
    "(Sidenote: there exists even more dedicated hardware specifically for the linear algebra operations needed to train and run neural networks. Modern high-end GPUs often include so-called tensor cores for these applications and there are ever entire accelerators consisting of exclusively these cores called tensor processing units - TPUs, but for all intents and purposes, scaling to this hardware is very similar to scaling to GPU training from our perspective, even though there are many differences on a low level.)\n",
    "\n",
    "1) We cannot make our batches of data large enough.\n",
    "    * This is a problem because of how stochastic gradient descent works. \n",
    "    * Generally, using gradient descent to fit a model to some data, we calculate the gradients of the loss with respect to parameters in the model. We then use the \"direction\" of the gradients to change our parameters in a way that will reduce the loss if the exact same data is passed through the model again. \n",
    "    * *Stochastic* gradient descent incorporates the notion of mini batches, which are essentially subsets of the data. This has practical reasons (i.e. the entire data can usually not be processed at once), but also modeling reasons. Mini batches provide a noisy estimate of the gradients we would get when passing all of the data through the model. This would be suboptimal if our training data were excatly the same as future data the model will encounter, but that's not usually the case. So, getting noisy gradient estimates and parameter updates can act as regularization, preventing us from overfitting to the training data and enabling better generalization in the future. See below for an illustration\n",
    "    * However, there is a tradeoff. Larger batchsizes mean our gradient estimate is closer to the \"true\" gradients of the training data but are harder to process and may lead to overfitting, while smaller batches increase the noise. If the batch is too small (e.g. a single sample), it is easy to see how the gradient estimates now become almost meaningless and training is near impossible. \n",
    "    * Furthermore, some applications require very large batch sizes, e.g. contrastive learning frameworks such as [SimCLR](https://arxiv.org/abs/2002.05709).\n",
    "    * One way to deal with this is gradient accumulation, where mini batches are essentially further broken up into micro batches, and multiple of those are passed through the model before and update to the parameters is made. However, this is still very slow as micro batches cannot be processed in parallel. \n",
    "2) We can't even fit the entire model into GPU memory.\n",
    "    * This is an obvious problem because it defeats the purpose of using a GPU in the first place - doing the matrix computations in as close to one big step as possible. \n",
    "    * This can be solved through model-parallelism, splitting the model onto multiple GPUs, each only holding a subset of all model parameters and performing a subset of the whole computation. \n",
    "\n",
    "At the end of the tutorial, there will be more info on how to train on GPUs, however, since this requires having access to a GPU, it will be tabled for now. The following examples generally make more sense in the context of scaling to multiple GPUs, but to keep it accessible, they will be kept on CPU to illustrate the basic principles before requiring specific hardware. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/sgd_illustration.png\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data parallelism\n",
    "* One way to solve problem 1) while still allowing for maximum parallel computation is (distributed) data parallelism.\n",
    "* Data parallelism is also orthogonal to model parallelism, so for large models, both can be applied simultaneously given enough hardware is available. \n",
    "* Let's make some assumptions to motivate data parallelism. \n",
    "    1) Assume, we're running on a machine with access to multiple GPUs or even have access to multiple machines with one or more GPUs.\n",
    "    2) Assume we can fit the model on a single GPU (or have enough hardware to apply model parallelism).\n",
    "    3) Assume we can only process very small batches of data on a single GPU (e.g. 8 examples).\n",
    "    \n",
    "**We want to efficiently use all of the available GPUs, possibly across multiple machines, to train the model as efficiently as possible.**\n",
    "\n",
    "(Sidenote: PyTorch has a data parallel (DP) and a distributed data parallel (DDP) algorithm. While DP is simpler, its performance suffers from language-inherent - GIL - and implementation-specific limitations compared to DDP. Additionally, DDP scales to settings with multiple machines where DP does not. These differences come down to DP chosing an easy-to-implement multithreading approach where DDP uses multiprocessing and IPC. Thus, principles from DDP easily generalize to DP but not vice versa and as such, DDP is more interesting and will be examined here.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/data_parallel_basic.png\" width=\"60%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point, torch.distributed must be supported for your OS, OS X for example does not currently support this.\n",
    "# That's why using a docker container is helpful\n",
    "try: \n",
    "    assert torch.distributed.is_available()\n",
    "except AssertionError:\n",
    "    raise NotImplementedError('OS does not support torch distributed training.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Data Parallel\n",
    "* The goal is to have a copy of our model on multiple devices, where each device can process an independent chunk of data and models get synchronizes at some point between learning steps.\n",
    "* Two approaches to achieving this may come to mind:\n",
    "    1) **Parameter averaging**: each model goes through a full training step locally, including updating its parameters, and then the synchronization consists of averaging all model parameters. This suffers from multiple drawbacks\n",
    "        * The synchronization steps and the training steps are completely separate and cannot be parallelized. All computation happens locally per copy of the model and then a separate communication step has to occur to synchroniza all models. Ideally, we would like the overlap these steps for increased efficiency. \n",
    "        * Mathematical equivalence to single-machine training is not guaranteed. Ideally, we would like the model resulting from parallelized training to be indetical to the model if it had hypothetically been trained on a single machine without any parallelism. This is not the case with parameter averaging for several reasons, most obviously because of local optimizer states may diverge between machines. Briefly, some optimizers maintain states that depend on gradients from previous epochs. If gradients are calculated locally per-model, they will differ because different copies of the model encounter different data. For instance, momentum, which helps with learning, may quickly diverge between optimizers depending on local gradients, resulting in an ultimately different model from what would have been with a single machine and a single optimizer. \n",
    "    2) **Gradient averaging**: After each backward pass, when all parameter gradients are calculated but BEFORE local model copies are updated, we can communicate all local gradients between the models and average them such that the updates done to all local models are the same after the update. \n",
    "        * This will take care of the issue with mathematical equivalency because optimizer states no longer diverge and we will see later how it also allows for some parallelized communication and computation. \n",
    "### A naive approach\n",
    "* To start, lets try to implement a first-pass, naive version of DDP which will illustrate the basic principles without getting too hung up on optimization details. \n",
    "* This section will use some concepts from high performance computing topics, mainly inter-process communication (IPC). Most of this is abstracted into frameworks (MPI for example) so a detailed knowledge of these concepts is not required. \n",
    "* The code here will be shown in the notebook, but will have to be run from scripts because it relies on spawning groups of separate python processes which is tricky (not possible?) from inside IPython. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup - initialize the processes:\n",
    "* First, to set up for DDP training, we need to launch multiple processes that can communicate with each other and all have a copy of the same model\n",
    "* Adding them to the same process group in the pytorch multiprocessing module allows for easy communication through establishes IPC software\n",
    "* Different IPC libraries have different advantages\n",
    "    * We use gloo here because its binaries are included with the pytorch installation\n",
    "\n",
    "```python\n",
    "def init_process(rank: int,\n",
    "                 world_size: int,\n",
    "                 train_fn: Callable,\n",
    "                 train_fn_kwargs: Optional[dict] = None,\n",
    "                 backend: Optional[str] = 'gloo'):\n",
    "    \"\"\"Function called to initialize individual processes\n",
    "\n",
    "    Args:\n",
    "        rank (int): Rank of this process\n",
    "        world_size (int): Number of processes in the group\n",
    "        train_fn (callable): Function that actually runs model training\n",
    "        backend (optional, str): Backend to use. Options are gloo, mpi, nccl. Default is gloo.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        val_backends = ['gloo', 'mpi', 'nccl']\n",
    "        assert backend in ['gloo', 'mpi', 'nccl']\n",
    "    except AssertionError:\n",
    "        raise NotImplementedError(\n",
    "            f'Supported backends: {val_backends} don\\'t include: {backend}')\n",
    "    # There are other ways than environment variables of initializing such as TCP or a shared file system for IPC\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '8899'\n",
    "    dist.init_process_group(\n",
    "        backend,\n",
    "        rank=rank,\n",
    "        world_size=world_size)\n",
    "    if train_fn_kwargs:\n",
    "        train_fn(rank, world_size, **train_fn_kwargs)\n",
    "    else:\n",
    "        train_fn(rank, world_size)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-s', \"--world_size\", type=int, default=2,\n",
    "                        help=\"number of processes to spawn\")\n",
    "    parser.add_argument('-i', \"--input_size\", type=int, default=100,\n",
    "                        help=\"dimensionality of the data\")\n",
    "    parser.add_argument('-n', \"--num_samples\", type=int, default=1_024,\n",
    "                        help=\"size of the dataset\")\n",
    "    parser.add_argument('-d', \"--num_hl\", type=int, default=0,\n",
    "                        help=\"number of hidden layers in the network\")\n",
    "    parser.add_argument('-z', \"--hl_size\", type=int, default=100,\n",
    "                        help=\"size of the hidden layers in the network\")\n",
    "    parser.add_argument('-e', \"--epochs\", type=int, default=10,\n",
    "                        help=\"number of epochs to train for\")\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    train_fn_kwargs = {\n",
    "        'input_size': args.input_size,\n",
    "        'num_samples': args.num_samples,\n",
    "        'num_hl': args.num_hl,\n",
    "        'hl_size': args.hl_size,\n",
    "        'epochs': args.epochs\n",
    "    }\n",
    "\n",
    "    processes = []\n",
    "    for rank in range(args.world_size):\n",
    "        p = mp.Process(target=init_process, args=(\n",
    "            rank, args.world_size, run_training, train_fn_kwargs, 'gloo'))\n",
    "        p.start()\n",
    "        processes.append(p)\n",
    "\n",
    "    for p in processes:\n",
    "        p.join()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup - partitioning the data across processes:\n",
    "* Now we need a way to partition the dataset and distribute it between all processes in our process group.\n",
    "* The partitioner takes the total dataset and then selects a random, non-overlapping subset for each process.\n",
    "\n",
    "```python\n",
    "class DatasetPartition(object):\n",
    "    \"\"\"Essentially the same as ToyDataset class.\n",
    "    Implements basic functionality for torch dataset on partitions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data: Iterable,\n",
    "                 index: int):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data (Iterable): some local data subset from global dataset\n",
    "            index (int): some index for local data\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.index = index\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns total size of this dataset\n",
    "\n",
    "        Returns:\n",
    "            int: total length\n",
    "        \"\"\"\n",
    "        return len(self.index)\n",
    "\n",
    "    def __getitem__(self,\n",
    "                    idx: int) -> Iterable:\n",
    "        \"\"\"_summary_\n",
    "\n",
    "        Args:\n",
    "            idx (int): index for item to getch\n",
    "\n",
    "        Returns:\n",
    "            Iterable: data at index\n",
    "        \"\"\"\n",
    "        local_index = self.index[idx]\n",
    "        return self.data[local_index]\n",
    "\n",
    "\n",
    "class DataPartitioner(object):\n",
    "    \"\"\"Helper class to go from global dataset to local partitions for each process.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 data: Iterable,\n",
    "                 world_size: int,\n",
    "                 seed: int = 7):\n",
    "        \"\"\"        \n",
    "        Args:\n",
    "            data (Iterable): Global dataset to be distributed among processes\n",
    "            world_size (int): Number of processes among which to distribute the data\n",
    "            seed (int, optional): Seed for random generator to ensure reproducability. Defaults to 7.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.partitions = []\n",
    "        rng = Random()\n",
    "        rng.seed(seed)\n",
    "\n",
    "        # list of all data indices to shuffle for random distribution\n",
    "        indices = list(range(0, len(data)))\n",
    "        rng.shuffle(indices)\n",
    "\n",
    "        # might drop modulo if not perfectly divisible for simplicity\n",
    "        part_len = len(data) // world_size\n",
    "        for _ in range(world_size):\n",
    "            self.partitions.append(indices[:part_len])\n",
    "            indices = indices[part_len:]\n",
    "\n",
    "    def use(self, partition_idx: int) -> DatasetPartition:\n",
    "        \"\"\"Return partition at index partition_idx\n",
    "\n",
    "        Args:\n",
    "            partitio_idx (int): partition index\n",
    "\n",
    "        Returns:\n",
    "            DatasetPartition: Partition for process partition_index\n",
    "        \"\"\"\n",
    "        return DatasetPartition(self.data, self.partitions[partition_idx])\n",
    "\n",
    "\n",
    "def partition_dataset(world_size: int,\n",
    "                      batch_size: int,\n",
    "                      input_size: int = 100,\n",
    "                      num_samples: int = 1_024) -> torch.utils.data.DataLoader:\n",
    "    \"\"\"Function actually splitting the dataset\n",
    "\n",
    "    Args:\n",
    "        world_size (int): Number of processes in the group\n",
    "        batch_size (int): Batch size per process\n",
    "        input_size (int, optional): Data dimensionality. Defaults to 100.\n",
    "        num_samples (int, optional): Total dataset size. Defaults to 1_024.\n",
    "\n",
    "    Returns:\n",
    "        torch.utils.data.DataLoader: Dataloader for this process' subset of the total data\n",
    "    \"\"\"\n",
    "    batch_size\n",
    "\n",
    "    train_ds = ToyDataset(input_size=input_size, num_samples=num_samples)\n",
    "    partition = DataPartitioner(train_ds, world_size)\n",
    "    partition = partition.use(dist.get_rank())\n",
    "    train_set = torch.utils.data.DataLoader(partition,\n",
    "                                            batch_size=batch_size,\n",
    "                                            shuffle=True)\n",
    "    return train_set\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup - gradient all_reduce:\n",
    "* Now we need to work on the IPC for gradient averaging\n",
    "* Luckily, since we already have a process group established we can make use of a standard all_reduce call in the pytorch distributed module to make this process as easy as possible\n",
    "\n",
    "```python\n",
    "\n",
    "def reduce_gradients(model: torch.nn.Module):\n",
    "    \"\"\"Function to all_reduce gradients from all processes\n",
    "\n",
    "    First gets and sums all gradients for each parameter and then averages them.\n",
    "    Stores the updated gradient information directly in the model so no need to return anything.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): Model being trained\n",
    "    \"\"\"\n",
    "    # number of total processes for averaging\n",
    "    size = float(dist.get_world_size())\n",
    "\n",
    "    # repeat this process for each individual parameter in the model\n",
    "    for param in model.parameters():\n",
    "        # get this parameter from each process' model copy and them sum them all\n",
    "        dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)\n",
    "        # divide by number of total processes to get the final gradient\n",
    "        param.grad.data /= size\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup - the main training loop:\n",
    "* This version of the training loop is very similar to the non-parallelized one with some essential differences:\n",
    "    * Each process only trains on its subset of the total dataset\n",
    "    * After the ```loss.backward()``` call - where local gradients are calculated -, we don't immediately call ```optimizer.step()``` - where parameters are updated - but instead call our function to average all gradients across the process group first. \n",
    "* Also note that every process still initializes a separate model but we use the same RNG seed in all processes so the models will be identical. Another way this could be accomplished is by having one process (e.g. rank 0) broadcast its parameters to all other processes. This would add a one-time communication overhead but overall be negligible extra effort. \n",
    "\n",
    "```python\n",
    "\n",
    "def run_training(rank: int,\n",
    "                 world_size: int,\n",
    "                 input_size: int = 100,\n",
    "                 num_samples: int = 1_024,\n",
    "                 num_hl: int = 0,\n",
    "                 hl_size: int = 10,\n",
    "                 epochs: int = 10):\n",
    "    \"\"\"Main training code\n",
    "\n",
    "    Args:\n",
    "        rank (int): Process rank\n",
    "        world_size (int): Total number processes in the group\n",
    "        input_size (int, optional): Data dimensionality, must be 1D. Defaults to 100.\n",
    "        num_samples (int, optional): Total number of samples in the dataset. Defaults to 1_024.\n",
    "        num_hl (int, optional): Number of hidden layers in the NN. Defaults to 0.\n",
    "        hl_size (int, optional): Size of the hidden layers in the NN. Defaults to 10.\n",
    "        epochs(int, optional): Number of epochs to train for.\n",
    "    \"\"\"\n",
    "    # By manually seeding we ensure all models start from the same initial state\n",
    "    # Another way of accomplishing this is by having one process (e.g. rank 0) broadcast its model at the start\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    batch_size = 8\n",
    "    train_set = partition_dataset(\n",
    "        world_size, batch_size, input_size, num_samples)\n",
    "\n",
    "    nn_mdl = FFNet(input_size=input_size, num_hl=num_hl, hl_size=hl_size)\n",
    "    optimizer = optim.SGD(nn_mdl.parameters(), lr=0.001, momentum=0.9)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.\n",
    "        for samples, labels in train_set:\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = nn_mdl(samples)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # this time we first have to average the gradients across the process group\n",
    "            reduce_gradients(nn_mdl)\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        print(\n",
    "            f'Process {rank} / {world_size} -> Epoch: {epoch + 1} - total loss: {running_loss / 2000:.3f}')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To run this script, simply run:\n",
    "* ```python ./ddp_tutorial/naive_ddp.py``` or ```time python ./ddp_tutorial/naive_ddp.py``` to time execution speed.\n",
    "\n",
    "* You can play around with the model and data sizes using flags like ```time python ./ddp_tutorial/native_ddp.py -d 3``` to increase the number of hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Several problems with this implementation:\n",
    "* You may notice that, when running this on a CPU along for larger networks, it actually performs worse than just running in sequence. On one hand, this has to do with the fact that the libraries are already optimized for training on multi-core CPUs, this would be different if we had multiple GPUs available and could distribute between those. But there are also implementation-specific reasons that would hinder performance regardless which are worth looking at. \n",
    "* Previously, when discussing parameter averaging, we noted that it removes the opportunity to overlay computation and communication\n",
    "    1) This implementation also does not take advantage of this opportunity, first calculating all gradients in ```loss.backward()``` then reducing them, and then updating the model\n",
    "    2) Also, reducing the parameters 1-by-1 is, as it turns out, not optimal because all_reduce is more efficient for large tensors. See below for a detailed analysis on the gloo library.\n",
    "* At this point, we will forgo implementing solutions for these problems ourselves and just cover them theoretically. While not easy in practice, it should be clear where to insert the optimizations discussed below into the naive framework established above. \n",
    "* The actual pytorch DDP package employs these and some more subtle optimizations and provides an easy interface to use them so we don't have to worry about doing it ourselves!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/allreduce_gloo.png\" width=\"40%\"/>\n",
    "<!--cite [1] https://arxiv.org/abs/2006.15704-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bucketing\n",
    "* By not waiting for the entire backward pass to complete, we can start communicating gradients as their calculations complete.\n",
    "* We don't want to communicate every gradient individually however, because we know doing all_reduce on larger tensors is more efficient\n",
    "* The solution is **bucketing**, grouping parameters into buckets\n",
    "    * The same parameters contain the same parameters in all processes\n",
    "    * Once a all gradients for a bucket are calculated in all processes, all_reduce is initiated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/torch_bucketing.png\" width=\"70%\"/>\n",
    "<!--cite [4] https://user-images.githubusercontent.com/16999635/72401724-d296d880-371a-11ea-90ab-737f86543df9.png-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bucketing (cont.)\n",
    "* As mentioned above, buckets must be synchronized, bucket 1 must be reduced with bucket 1, not bucket 0. This introduces the challenge of figuring out a bucketing order that is as efficient as possible, otherwise, if P1-bucket1 finishes first, and P0-bucket1 finishes last, we have gained nothing.\n",
    "* Fortunately, the way gradients are calculated in a deep neural network interacts extremely well with this requirement. \n",
    "* Generally, we can think of the gradients as being calcuated in the reverse order of how data flows through the network, with the highest parameters in the network first getting their gradients. In practice, this is much more complicated, especially for networks with separate computational block and residual connections, but this will suffice for an intuition.\n",
    "* The bucket order can thus simply be established in the order in which parameters appear in the network. Higher parameters are grouped with each other, and similarly for lower parameters. Thus, we somewhat assure that buckets complete computation in similar order across processes and can be reduces quickly.\n",
    "* An example is shown below, where arrows point in the direction of the ```.backward()``` call, the order in which gradients are calculated. \n",
    "* While bucket size only has a moderate performance input, it is an easy-to-adjust performance lever and provides an interesting trade-off:\n",
    "    * Larger buckets contain larger gradient tensors making all_reduce more efficient.\n",
    "    * Larger buckets, however, also take longer to become ready, so communication can't be initiated as quickly and opportunities for asynchronous processing are lost. \n",
    "    * Ultimately, if performance is critical, this should be tuned empirically, as it depends on hardware and what IPC libraries are used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./images/ddp_full.png\" width=\"70%\"/>\n",
    "<!--cite [1] https://arxiv.org/abs/2006.15704-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A working, full DDP example:\n",
    "* Now that the most important concepts have been covered, let's look at a working example of the full PyTorch DDP implementation in action:\n",
    "    * At this point, we will train on GPUs, not needlessly split the workload among a single CPU. \n",
    "    * Each process will be associated with its own GPU.\n",
    "\n",
    "```python\n",
    "\n",
    "def run_training(gpu: int,\n",
    "                 args: dict):\n",
    "    \"\"\"Main training code\n",
    "\n",
    "    Args:\n",
    "        gpu (int): index of the gpu for this process on this process' node\n",
    "        args (dict): dictionary of args\n",
    "    \"\"\"\n",
    "    # By manually seeding we ensure all models start from the same initial state\n",
    "    # Another way of accomplishing this is by having one process (e.g. rank 0) broadcast its model at the start\n",
    "    torch.manual_seed(0)\n",
    "\n",
    "    batch_size = 8\n",
    "    rank = args.nr * args.gpus + gpu\n",
    "    dist.init_process_group(\n",
    "        backend='gloo',\n",
    "        world_size=args.world_size,\n",
    "        rank=rank\n",
    "    )\n",
    "\n",
    "    # Initialize the model on the GPU\n",
    "    torch.cuda.set_device(gpu)\n",
    "    nn_mdl = FFNet(input_size=args.input_size,\n",
    "                   num_hl=args.num_hl, hl_size=args.hl_size)\n",
    "    nn_mdl.cuda(gpu)\n",
    "\n",
    "    # Optimizer and loss (loss has to be put on GPU too)\n",
    "    optimizer = optim.SGD(nn_mdl.parameters(), lr=0.001, momentum=0.9)\n",
    "    loss_fn = nn.MSELoss().cuda(gpu)\n",
    "\n",
    "    # Distributing the model is as easy as that\n",
    "    model = nn.parallel.DistributedDataParallel(model,\n",
    "                                                device_ids=[gpu])\n",
    "\n",
    "    # Initialize the dataset\n",
    "    train_ds = ToyDataset(input_size=args.input_size,\n",
    "                          num_samples=args.num_samples)\n",
    "\n",
    "    # This will communicate to the process which subset of the dataset to draw from\n",
    "    # Essentially the partition helper from the naive example\n",
    "    train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
    "        train_ds,\n",
    "        num_replicas=args.world_size,\n",
    "        rank=rank\n",
    "    )\n",
    "\n",
    "    # torch dataloader interfaces nicely with the distributed sampler\n",
    "    train_set = torch.utils.data.DataLoader(\n",
    "        dataset=train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        sampler=train_sampler)\n",
    "\n",
    "    for epoch in range(args.epochs):\n",
    "        running_loss = 0.\n",
    "        for samples, labels in train_set:\n",
    "            # data has to be loaded to the gpu\n",
    "            samples.cuda(non_blocking=True)\n",
    "            labels.cuda(non_blocking=True)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # forward + backward + optimize\n",
    "            outputs = nn_mdl(samples)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            # at this point we previously had to average the gradients\n",
    "            # torch native DDP uses hooks on the autograd graph to do this in the backend\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "        if args.verbose:\n",
    "            print(\n",
    "                f'Process {rank} / {args.world_size} -> Epoch: {epoch + 1} - total loss: {running_loss / 2000:.3f}')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-n', '--nodes', default=1,\n",
    "                        type=int, help='number of nodes')\n",
    "    parser.add_argument('-g', '--gpus', default=1, type=int,\n",
    "                        help='number of gpus per node')\n",
    "    parser.add_argument('-s', \"--world_size\", type=int, default=2,\n",
    "                        help=\"number of processes to spawn\")\n",
    "    parser.add_argument('-i', \"--input_size\", type=int, default=100,\n",
    "                        help=\"dimensionality of the data\")\n",
    "    parser.add_argument('-nb', \"--num_samples\", type=int, default=1_024,\n",
    "                        help=\"size of the dataset\")\n",
    "    parser.add_argument('-d', \"--num_hl\", type=int, default=0,\n",
    "                        help=\"number of hidden layers in the network\")\n",
    "    parser.add_argument('-z', \"--hl_size\", type=int, default=100,\n",
    "                        help=\"size of the hidden layers in the network\")\n",
    "    parser.add_argument('-e', \"--epochs\", type=int, default=10,\n",
    "                        help=\"number of epochs to train for\")\n",
    "    parser.add_argument('-nr', '--nr', default=0, type=int,\n",
    "                        help='ranking within the nodes')\n",
    "    parser.add_argument('-v', '--verbose', type=int, default=1,\n",
    "                        help='Print training process. 1=verbose, 0=silent.')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # one process per GPU, nodes could be scaled too but not on the UH cluster since multi-node-gpu jobs are prohibited\n",
    "    args.world_size = args.gpus * args.nodes\n",
    "    os.environ['MASTER_ADDR'] = '127.0.0.1'\n",
    "    os.environ['MASTER_PORT'] = '8899'\n",
    "    mp.spawn(run_training, nprocs=args.gpus, args=(args,))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A working, full DDP example (cont.):\n",
    "* This script is obviously much shorts, but note how it still works very similar to the naive approach.\n",
    "* ```nn.parallal.DistributedDataParallel``` takes care of coordinating the model copies, while the ```DistributedSampler``` provides an elegant way of partitioning the dataset. \n",
    "* The only difference is the lack of an explicit all_reduce call in the code. This is because ```DistributedDataParallel``` moves this to the backend, including grouping parameters into buckets and coordinating all_reduce calls accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To run this script:\n",
    "* If you are on the UH cluster, you can use the batch script in ```./ddp_tutorial/full_ddp.sh```, simply adapt the paths and you should be good to go\n",
    "* If you running on a local machine with multiple GPUs, you can invoke it as before using ```python ./ddp_tutorial/full_ddp.py``` or ```time python ./ddp_tutorial/full_ddp.py``` to time execution speed.\n",
    "* Again, feel free to change the flags and see how it affects performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources:\n",
    "* There is lots more to explore in terms of DDP and scaling deep learning in general, much more than can be mentioned here.\n",
    "* Some natural extension to what was covered here however:\n",
    "    * [PyTorch distributed RPC framework](https://pytorch.org/docs/stable/rpc.html): this is a more flexible framework for distributed training that extends well beyond data parallelism alone.\n",
    "    * [PyTorch automatic mixed precision](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html): this is an excellent way of further scaling up training. If you have access to a device with tensor cores, this allows for storing some tensors as fp16, saving memory, speeding up computation, and allowing you to process more data at once!\n",
    "    * [PyTorch Lightning](https://www.pytorchlightning.ai/) is an excellent high-level wrapper around PyTorch (think keras for tensorflow). It's build for research purposes, and, as it happens, also supports [distributed training](https://pytorch-lightning.readthedocs.io/en/stable/accelerators/gpu_intermediate.html). Scaling lightning apps to distributed training is even easier than it is in native PyTorch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources:\n",
    "* These are the sources I used throughout this post, a lot of the code is adapted from here and you can find much additional information on there!\n",
    "\n",
    "Primary paper:\n",
    "1) https://arxiv.org/abs/2006.15704\n",
    "\n",
    "Secondary websites/ blog posts:\n",
    "1) https://pytorch.org/docs/stable/notes/ddp.html\n",
    "2) https://pytorch.org/tutorials/intermediate/ddp_tutorial.html\n",
    "3) https://pytorch.org/tutorials/intermediate/dist_tuto.html#collective-communication\n",
    "4) http://seba1511.net/dist_blog/\n",
    "5) https://yangkky.github.io/2019/07/08/distributed-pytorch-tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ssl_demo')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1fece7c4f4adb239025c48c3ec00e743632c1c3caa6a6cb39a39f7bcaab8c3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
